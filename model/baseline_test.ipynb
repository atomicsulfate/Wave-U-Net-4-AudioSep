{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, argparse\n",
    "import time\n",
    "from functools import partial\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# add project root dir to sys.path so that all packages can be found by python.\n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.realpath(\"__file__\")))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_params import ModelArgs\n",
    "from model.waveunet_params import waveunet_params\n",
    "import model.utils as model_utils\n",
    "import utils\n",
    "from data.dataset import SeparationDataset\n",
    "from data.musdb import get_musdb_folds\n",
    "from data.utils import crop_targets, random_amplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model.crop import centre_crop\n",
    "from model.resample import Resample1d\n",
    "from model.conv import ConvLayer\n",
    "\n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res, num_convs):\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        assert(stride > 1)\n",
    "        self.num_convs = num_convs\n",
    "        # CONV 1 for UPSAMPLING\n",
    "        if self.num_convs == 1:\n",
    "            if res == \"fixed\":\n",
    "                self.upconv = Resample1d(n_shortcut, 15, stride, transpose=True)\n",
    "            else:\n",
    "                self.upconv = ConvLayer(n_shortcut, n_shortcut, kernel_size, stride, conv_type, transpose=True)\n",
    "        else:\n",
    "            if res == \"fixed\":\n",
    "                self.upconv = Resample1d(n_inputs, 15, stride, transpose=True)\n",
    "            else:\n",
    "                self.upconv = ConvLayer(n_inputs, n_inputs, kernel_size, stride, conv_type, transpose=True)\n",
    "\n",
    "        if self.num_convs == 2:    \n",
    "            self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                    [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "            # CONVS to combine high- with low-level information (from shortcut)\n",
    "            self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_outputs + n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                     [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "        elif self.num_convs == 1:\n",
    "            self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_outputs + n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                    [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "    def forward(self, x, shortcut):\n",
    "        # UPSAMPLE HIGH-LEVEL FEATURES\n",
    "        upsampled = self.upconv(x)\n",
    "        \n",
    "        if self.num_convs == 2:\n",
    "            for conv in self.pre_shortcut_convs:\n",
    "                upsampled = conv(upsampled)\n",
    "\n",
    "        # Prepare shortcut connection\n",
    "        combined = centre_crop(shortcut, upsampled)\n",
    "\n",
    "        # Combine high- and low-level features\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            combined = conv(torch.cat([combined, centre_crop(upsampled, combined)], dim=1))\n",
    "        return combined\n",
    "\n",
    "    def get_output_size(self, input_size):\n",
    "        curr_size = self.upconv.get_output_size(input_size)\n",
    "\n",
    "        # Upsampling convs\n",
    "        if self.num_convs == 2:\n",
    "            for conv in self.pre_shortcut_convs:\n",
    "                curr_size = conv.get_output_size(curr_size)\n",
    "\n",
    "        # Combine convolutions\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            curr_size = conv.get_output_size(curr_size)\n",
    "\n",
    "        return curr_size\n",
    "\n",
    "class DownsamplingBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res, num_convs):\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        assert(stride > 1)\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.num_convs = num_convs\n",
    "\n",
    "        # CONV 1\n",
    "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_shortcut, kernel_size, 1, conv_type)] +\n",
    "                                                [ConvLayer(n_shortcut, n_shortcut, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.num_convs == 2:\n",
    "            self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in\n",
    "                                                  range(depth - 1)])\n",
    "\n",
    "        if self.num_convs == 1:\n",
    "            n_outputs = n_shortcut\n",
    "            \n",
    "        # CONV 2 with decimation\n",
    "        if res == \"fixed\":\n",
    "            self.downconv = Resample1d(n_outputs, 15, stride) # Resampling with fixed-size sinc lowpass filter\n",
    "        elif res == \"naive\":\n",
    "            #todo: add decimation here\n",
    "            self.downconv = self.naive_decimation\n",
    "        else:\n",
    "            self.downconv = ConvLayer(n_outputs, n_outputs, kernel_size, stride, conv_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PREPARING SHORTCUT FEATURES\n",
    "        shortcut = x\n",
    "        for conv in self.pre_shortcut_convs:\n",
    "            shortcut = conv(shortcut)\n",
    "        \n",
    "        # PREPARING FOR DOWNSAMPLING\n",
    "        out = shortcut\n",
    "        if self.num_convs == 2:\n",
    "            for conv in self.post_shortcut_convs:\n",
    "                out = conv(out)\n",
    "\n",
    "        # DOWNSAMPLING\n",
    "        out = self.downconv(out)\n",
    "\n",
    "        return out, shortcut\n",
    "\n",
    "    def get_input_size(self, output_size):\n",
    "        curr_size = self.downconv.get_input_size(output_size)\n",
    "        if self.num_convs == 2:\n",
    "            for conv in reversed(self.post_shortcut_convs):\n",
    "                curr_size = conv.get_input_size(curr_size)\n",
    "\n",
    "        for conv in reversed(self.pre_shortcut_convs):\n",
    "            curr_size = conv.get_input_size(curr_size)\n",
    "        return curr_size\n",
    "\n",
    "    def naive_decimation(self, x):\n",
    "        # a very naive decimation\n",
    "        return x[:,::2,:] # Decimate by factor of 2 # out = (in-1)/2 + 1\n",
    "\n",
    "class Waveunet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, num_outputs, instruments, upsampling_kernel_size, downsampling_kernel_size, bottleneck_kernel_size, target_output_size, conv_type, res, separate=False, depth=1, strides=2, num_convs=2):\n",
    "        super(Waveunet, self).__init__()\n",
    "\n",
    "        self.num_levels = len(num_channels)\n",
    "        self.strides = strides\n",
    "        self.upsampling_kernel_size = upsampling_kernel_size\n",
    "        self.downsampling_kernel_size = downsampling_kernel_size\n",
    "        self.bottleneck_kernel_size = bottleneck_kernel_size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.depth = depth\n",
    "        self.instruments = instruments\n",
    "        self.separate = separate\n",
    "        self.num_convs=num_convs\n",
    "\n",
    "         # Only odd filter kernels allowed\n",
    "        assert(downsampling_kernel_size % 2 == 1)\n",
    "        assert(bottleneck_kernel_size % 2 == 1)\n",
    "        assert(upsampling_kernel_size % 2 == 1)\n",
    "\n",
    "        self.waveunets = nn.ModuleDict()\n",
    "\n",
    "        model_list = instruments if separate else [\"ALL\"]\n",
    "        # Create a model for each source if we separate sources separately, otherwise only one (model_list=[\"ALL\"])\n",
    "        for instrument in model_list:\n",
    "            module = nn.Module()\n",
    "\n",
    "            module.downsampling_blocks = nn.ModuleList()\n",
    "            module.upsampling_blocks = nn.ModuleList()\n",
    "\n",
    "            for i in range(self.num_levels - 1):\n",
    "                in_ch = num_inputs if i == 0 else num_channels[i]\n",
    "\n",
    "                module.downsampling_blocks.append(\n",
    "                    DownsamplingBlock(in_ch, num_channels[i], num_channels[i+1], self.downsampling_kernel_size, strides, depth, conv_type, res, num_convs))\n",
    "\n",
    "            for i in range(0, self.num_levels - 1):\n",
    "                module.upsampling_blocks.append(\n",
    "                    UpsamplingBlock(num_channels[-1-i], num_channels[-2-i], num_channels[-2-i], self.upsampling_kernel_size, strides, depth, conv_type, res, num_convs))\n",
    "\n",
    "            if self.num_convs == 2:\n",
    "                module.bottlenecks = nn.ModuleList(\n",
    "                    [ConvLayer(num_channels[-1], num_channels[-1], self.bottleneck_kernel_size, 1, conv_type) for _ in range(depth)])\n",
    "            elif self.num_convs == 1:\n",
    "                module.bottlenecks = nn.ModuleList(\n",
    "                    [ConvLayer(num_channels[i], num_channels[i], self.bottleneck_kernel_size, 1, conv_type) for _ in range(depth)])\n",
    "        \n",
    "            # Output conv\n",
    "            outputs = num_outputs if separate else num_outputs * len(instruments)\n",
    "            module.output_conv = nn.Conv1d(num_channels[0], outputs, 1)\n",
    "\n",
    "            self.waveunets[instrument] = module\n",
    "\n",
    "        self.set_output_size(target_output_size)\n",
    "\n",
    "    def set_output_size(self, target_output_size):\n",
    "        self.target_output_size = target_output_size\n",
    "\n",
    "        self.input_size, self.output_size = self.check_padding(target_output_size)\n",
    "        print(\"Using valid convolutions with \" + str(self.input_size) + \" inputs and \" + str(self.output_size) + \" outputs\")\n",
    "\n",
    "        assert((self.input_size - self.output_size) % 2 == 0)\n",
    "        self.shapes = {\"output_start_frame\" : (self.input_size - self.output_size) // 2,\n",
    "                       \"output_end_frame\" : (self.input_size - self.output_size) // 2 + self.output_size,\n",
    "                       \"output_frames\" : self.output_size,\n",
    "                       \"input_frames\" : self.input_size}\n",
    "\n",
    "    def check_padding(self, target_output_size):\n",
    "        # Ensure number of outputs covers a whole number of cycles so each output in the cycle is weighted equally during training\n",
    "        bottleneck = 1\n",
    "\n",
    "        while True:\n",
    "            out = self.check_padding_for_bottleneck(bottleneck, target_output_size)\n",
    "            if out is not False:\n",
    "                return out\n",
    "            bottleneck += 1\n",
    "\n",
    "    def check_padding_for_bottleneck(self, bottleneck, target_output_size):\n",
    "        module = self.waveunets[[k for k in self.waveunets.keys()][0]]\n",
    "        try:\n",
    "            curr_size = bottleneck\n",
    "            for idx, block in enumerate(module.upsampling_blocks):\n",
    "                curr_size = block.get_output_size(curr_size)\n",
    "            output_size = curr_size\n",
    "\n",
    "            # Bottleneck-Conv\n",
    "            curr_size = bottleneck\n",
    "            for block in reversed(module.bottlenecks):\n",
    "                curr_size = block.get_input_size(curr_size)\n",
    "            for idx, block in enumerate(reversed(module.downsampling_blocks)):\n",
    "                curr_size = block.get_input_size(curr_size)\n",
    "\n",
    "            assert(output_size >= target_output_size)\n",
    "            return curr_size, output_size\n",
    "        except AssertionError as e:\n",
    "            return False\n",
    "\n",
    "    def forward_module(self, x, module):\n",
    "        '''\n",
    "        A forward pass through a single Wave-U-Net (multiple Wave-U-Nets might be used, one for each source)\n",
    "        :param x: Input mix\n",
    "        :param module: Network module to be used for prediction\n",
    "        :return: Source estimates\n",
    "        '''\n",
    "        shortcuts = []\n",
    "        out = x\n",
    "\n",
    "        # DOWNSAMPLING BLOCKS\n",
    "        for block in module.downsampling_blocks:\n",
    "            out, short = block(out)\n",
    "            shortcuts.append(short)\n",
    "\n",
    "        # BOTTLENECK CONVOLUTION\n",
    "        for conv in module.bottlenecks:\n",
    "            out = conv(out)\n",
    "\n",
    "        # UPSAMPLING BLOCKS\n",
    "        for idx, block in enumerate(module.upsampling_blocks):\n",
    "            out = block(out, shortcuts[-1 - idx])\n",
    "\n",
    "        # OUTPUT CONV\n",
    "        out = module.output_conv(out)\n",
    "        if not self.training:  # At test time clip predictions to valid amplitude range\n",
    "            out = out.clamp(min=-1.0, max=1.0)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, inst=None):\n",
    "        curr_input_size = x.shape[-1]\n",
    "        assert(curr_input_size == self.input_size) # User promises to feed the proper input himself, to get the pre-calculated (NOT the originally desired) output size\n",
    "\n",
    "        if self.separate:\n",
    "            return {inst : self.forward_module(x, self.waveunets[inst])}\n",
    "        else:\n",
    "            assert(len(self.waveunets) == 1)\n",
    "            out = self.forward_module(x, self.waveunets[\"ALL\"])\n",
    "\n",
    "            out_dict = {}\n",
    "            for idx, inst in enumerate(self.instruments):\n",
    "                out_dict[inst] = out[:, idx * self.num_outputs:(idx + 1) * self.num_outputs]\n",
    "            return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.musdb_loader import setup_hq_musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_waveunet(args):\n",
    "    num_features = [args.features * i for i in range(1, args.levels + 1)] if args.feature_growth == \"add\" else \\\n",
    "        [args.features * 2 ** i for i in range(0, args.levels)]\n",
    "    target_outputs = int(args.output_size * args.sr)\n",
    "    model = Waveunet(args.channels, num_features, args.channels, args.instruments, downsampling_kernel_size=args.downsampling_kernel_size,\n",
    "                     upsampling_kernel_size=args.upsampling_kernel_size, bottleneck_kernel_size=args.bottleneck_kernel_size,\n",
    "                     target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
    "                     conv_type=args.conv_type, res=args.res, separate=args.separate, num_convs=args.num_convs)\n",
    "\n",
    "    if args.cuda:\n",
    "        model = model_utils.DataParallel(model)\n",
    "        print(\"move model to gpu\")\n",
    "        model.cuda()\n",
    "\n",
    "    print('model: ', model)\n",
    "    print('parameter count: ', str(sum(p.numel() for p in model.parameters())))\n",
    "    return model\n",
    "\n",
    "def _load_musdb(args, data_shapes):\n",
    "    # musdb = get_musdb_folds(args.dataset_dir, version=\"toy\")\n",
    "    musdb = None\n",
    "    # If not data augmentation, at least crop targets to fit model output shape\n",
    "    crop_func = partial(crop_targets, shapes=data_shapes)\n",
    "    # Data augmentation function for training\n",
    "    augment_func = partial(random_amplify, shapes=data_shapes, min=0.7, max=1.0)\n",
    "\n",
    "    test_data = SeparationDataset(musdb, \"test\", args.instruments, args.sr, args.channels, data_shapes, False,\n",
    "                                  args.hdf_dir, audio_transform=crop_func)\n",
    "    return test_data, musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = waveunet_params.get_defaults()\n",
    "args.instruments = [\"accompaniment\", \"vocals\"]\n",
    "args.sr=22050 \n",
    "args.channels= 1 \n",
    "args.output_size= 2 \n",
    "args.patience= 20 \n",
    "args.separate= 0 \n",
    "args.features= 24 \n",
    "args.lr= 1e-4 \n",
    "args.min_lr= 1e-4\n",
    "args.batch_size= 16 \n",
    "args.levels= 2  \n",
    "args.depth=1 \n",
    "args.kernel_size= 5 \n",
    "args.strides= 2 \n",
    "args.loss= \"L2\" \n",
    "args.conv_type= \"normal\" \n",
    "args.dataset_dir = '../data/musdb'\n",
    "args.hdf_dir = '../data/hdf/'\n",
    "args.num_convs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using valid convolutions with 44125 inputs and 44101 outputs\n",
      "model:  Waveunet(\n",
      "  (waveunets): ModuleDict(\n",
      "    (ALL): Module(\n",
      "      (downsampling_blocks): ModuleList(\n",
      "        (0): DownsamplingBlock(\n",
      "          (pre_shortcut_convs): ModuleList(\n",
      "            (0): ConvLayer(\n",
      "              (filter): Conv1d(1, 24, kernel_size=(5,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "          (post_shortcut_convs): ModuleList(\n",
      "            (0): ConvLayer(\n",
      "              (filter): Conv1d(24, 48, kernel_size=(5,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "          (downconv): Resample1d()\n",
      "        )\n",
      "      )\n",
      "      (upsampling_blocks): ModuleList(\n",
      "        (0): UpsamplingBlock(\n",
      "          (upconv): Resample1d()\n",
      "          (pre_shortcut_convs): ModuleList(\n",
      "            (0): ConvLayer(\n",
      "              (filter): Conv1d(48, 24, kernel_size=(5,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "          (post_shortcut_convs): ModuleList(\n",
      "            (0): ConvLayer(\n",
      "              (filter): Conv1d(48, 24, kernel_size=(5,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bottlenecks): ModuleList(\n",
      "        (0): ConvLayer(\n",
      "          (filter): Conv1d(48, 48, kernel_size=(5,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "      (output_conv): Conv1d(24, 2, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "parameter count:  30578\n"
     ]
    }
   ],
   "source": [
    "model = _create_waveunet(args)\n",
    "test_data, musdb = _load_musdb(args, model.shapes)\n",
    "sample_track = test_data[0]\n",
    "dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=True)\n",
    "for example_num, (x, targets) in enumerate(dataloader):\n",
    "    sample = (x, targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accompaniment': tensor([[[-0.0637, -0.0651, -0.0651,  ..., -0.0665, -0.0663, -0.0643]],\n",
       " \n",
       "         [[-0.0603, -0.0599, -0.0596,  ..., -0.0645, -0.0646, -0.0640]],\n",
       " \n",
       "         [[-0.0604, -0.0602, -0.0601,  ..., -0.0638, -0.0633, -0.0628]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0608, -0.0610, -0.0610,  ..., -0.0627, -0.0626, -0.0625]],\n",
       " \n",
       "         [[-0.0640, -0.0641, -0.0641,  ..., -0.0612, -0.0613, -0.0607]],\n",
       " \n",
       "         [[-0.0601, -0.0608, -0.0605,  ..., -0.0611, -0.0610, -0.0611]]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " 'vocals': tensor([[[0.0381, 0.0381, 0.0384,  ..., 0.0411, 0.0418, 0.0419]],\n",
       " \n",
       "         [[0.0361, 0.0360, 0.0359,  ..., 0.0451, 0.0451, 0.0455]],\n",
       " \n",
       "         [[0.0376, 0.0361, 0.0339,  ..., 0.0425, 0.0422, 0.0417]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0394, 0.0394, 0.0394,  ..., 0.0448, 0.0449, 0.0450]],\n",
       " \n",
       "         [[0.0448, 0.0448, 0.0449,  ..., 0.0345, 0.0345, 0.0348]],\n",
       " \n",
       "         [[0.0387, 0.0393, 0.0394,  ..., 0.0401, 0.0397, 0.0392]]],\n",
       "        grad_fn=<SliceBackward0>)}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample[0], inst=\"vocals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = waveunet_params.get_defaults()\n",
    "args.instruments = [\"accompaniment\", \"vocals\"]\n",
    "args.sr=22050 \n",
    "args.channels= 1 \n",
    "args.output_size= 2 \n",
    "args.patience= 20 \n",
    "args.separate= 0 \n",
    "args.features= 24 \n",
    "args.lr= 1e-4 \n",
    "args.min_lr= 1e-4\n",
    "args.batch_size= 16 \n",
    "args.levels= 2  \n",
    "args.depth= 1 \n",
    "args.kernel_size= 5 \n",
    "args.strides= 2 \n",
    "args.loss= \"L2\" \n",
    "args.conv_type= \"normal\" \n",
    "args.dataset_dir = '../data/musdb'\n",
    "args.hdf_dir = '../data/hdf/'\n",
    "args.num_convs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using valid convolutions with 44117 inputs and 44101 outputs\n",
      "model:  Waveunet(\n",
      "  (waveunets): ModuleDict(\n",
      "    (ALL): Module(\n",
      "      (downsampling_blocks): ModuleList(\n",
      "        (0): DownsamplingBlock(\n",
      "          (pre_shortcut_convs): ModuleList(\n",
      "            (0): ConvLayer(\n",
      "              (filter): Conv1d(1, 24, kernel_size=(5,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "          (downconv): Resample1d()\n",
      "        )\n",
      "      )\n",
      "      (upsampling_blocks): ModuleList(\n",
      "        (0): UpsamplingBlock(\n",
      "          (upconv): Resample1d()\n",
      "          (post_shortcut_convs): ModuleList(\n",
      "            (0): ConvLayer(\n",
      "              (filter): Conv1d(48, 24, kernel_size=(5,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bottlenecks): ModuleList(\n",
      "        (0): ConvLayer(\n",
      "          (filter): Conv1d(24, 24, kernel_size=(5,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "      (output_conv): Conv1d(24, 2, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "parameter count:  9602\n"
     ]
    }
   ],
   "source": [
    "new_model = _create_waveunet(args)\n",
    "test_data, musdb = _load_musdb(args, new_model.shapes)\n",
    "sample_track = test_data[0]\n",
    "dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=True)\n",
    "for example_num, (x, targets) in enumerate(dataloader):\n",
    "    sample = (x, targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accompaniment': tensor([[[-0.1760, -0.1798, -0.1865,  ..., -0.1842, -0.1814, -0.1796]],\n",
       " \n",
       "         [[-0.1813, -0.1821, -0.1828,  ..., -0.1800, -0.1803, -0.1837]],\n",
       " \n",
       "         [[-0.1843, -0.1833, -0.1831,  ..., -0.1828, -0.1772, -0.1764]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.1829, -0.1825, -0.1831,  ..., -0.1830, -0.1829, -0.1831]],\n",
       " \n",
       "         [[-0.1822, -0.1829, -0.1828,  ..., -0.1825, -0.1824, -0.1827]],\n",
       " \n",
       "         [[-0.1830, -0.1830, -0.1830,  ..., -0.1830, -0.1830, -0.1830]]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " 'vocals': tensor([[[-0.1176, -0.1195, -0.1193,  ..., -0.1249, -0.1224, -0.1215]],\n",
       " \n",
       "         [[-0.1126, -0.1148, -0.1164,  ..., -0.1148, -0.1165, -0.1199]],\n",
       " \n",
       "         [[-0.1127, -0.1126, -0.1123,  ..., -0.1154, -0.1108, -0.1099]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.1170, -0.1162, -0.1168,  ..., -0.1131, -0.1132, -0.1131]],\n",
       " \n",
       "         [[-0.1126, -0.1131, -0.1145,  ..., -0.1182, -0.1173, -0.1172]],\n",
       " \n",
       "         [[-0.1155, -0.1156, -0.1155,  ..., -0.1155, -0.1156, -0.1155]]],\n",
       "        grad_fn=<SliceBackward0>)}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model(sample[0], inst=\"vocals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accompaniment': tensor([[[-0.0406, -0.0441, -0.0457,  ..., -0.0434, -0.0455, -0.0496]],\n",
       " \n",
       "         [[-0.0439, -0.0450, -0.0451,  ..., -0.0401, -0.0407, -0.0386]],\n",
       " \n",
       "         [[-0.0356, -0.0370, -0.0389,  ..., -0.0396, -0.0401, -0.0402]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0388, -0.0388, -0.0388,  ..., -0.0388, -0.0388, -0.0388]],\n",
       " \n",
       "         [[-0.0471, -0.0463, -0.0454,  ..., -0.0449, -0.0467, -0.0459]],\n",
       " \n",
       "         [[-0.0428, -0.0467, -0.0470,  ..., -0.0353, -0.0357, -0.0350]]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " 'vocals': tensor([[[0.1289, 0.1318, 0.1339,  ..., 0.1296, 0.1298, 0.1253]],\n",
       " \n",
       "         [[0.1293, 0.1255, 0.1246,  ..., 0.1303, 0.1252, 0.1268]],\n",
       " \n",
       "         [[0.1300, 0.1306, 0.1301,  ..., 0.1282, 0.1281, 0.1273]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.1285, 0.1285, 0.1285,  ..., 0.1285, 0.1285, 0.1285]],\n",
       " \n",
       "         [[0.1242, 0.1253, 0.1269,  ..., 0.1270, 0.1232, 0.1233]],\n",
       " \n",
       "         [[0.1288, 0.1270, 0.1249,  ..., 0.1315, 0.1315, 0.1292]]],\n",
       "        grad_fn=<SliceBackward0>)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 00:01:13.842063: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "np_tensor = sample[0].numpy()\n",
    "tf_tensor = tf.convert_to_tensor(np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tf_baseline\n",
    "from tf_baseline import LeakyReLU\n",
    "import numpy as np\n",
    "\n",
    "class UnetAudioSeparator:\n",
    "    '''\n",
    "    U-Net separator network for singing voice separation.\n",
    "    Uses valid convolutions, so it predicts for the centre part of the input - only certain input and output shapes are therefore possible (see getpadding function)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "        Initialize U-net\n",
    "        :param num_layers: Number of down- and upscaling layers in the network \n",
    "        '''\n",
    "        self.num_layers = (args.channels)\n",
    "        self.num_initial_filters = 44125\n",
    "        self.filter_size = args.kernel_size\n",
    "        self.merge_filter_size = args.kernel_size\n",
    "        self.input_filter_size = args.kernel_size\n",
    "        self.output_filter_size = args.kernel_size\n",
    "        self.upsampling = None\n",
    "        self.output_type = \"direct\"\n",
    "        self.context = None\n",
    "        self.padding = \"valid\" if self.context else \"same\"\n",
    "        self.source_names = args.instruments\n",
    "        self.num_channels = args.channels\n",
    "        self.output_activation = \"tanh\"\n",
    "\n",
    "    def get_padding(self, shape):\n",
    "        '''\n",
    "        Calculates the required amounts of padding along each axis of the input and output, so that the Unet works and has the given shape as output shape\n",
    "        :param shape: Desired output shape \n",
    "        :return: Input_shape, output_shape, where each is a list [batch_size, time_steps, channels]\n",
    "        '''\n",
    "\n",
    "        if self.context:\n",
    "            # Check if desired shape is possible as output shape - go from output shape towards lowest-res feature map\n",
    "            rem = float(shape[1]) # Cut off batch size number and channel\n",
    "\n",
    "            # Output filter size\n",
    "            rem = rem - self.output_filter_size + 1\n",
    "\n",
    "            # Upsampling blocks\n",
    "            for i in range(self.num_layers):\n",
    "                rem = rem + self.merge_filter_size - 1\n",
    "                rem = (rem + 1.) / 2.# out = in + in - 1 <=> in = (out+1)/\n",
    "\n",
    "            # Round resulting feature map dimensions up to nearest integer\n",
    "            x = np.asarray(np.ceil(rem),dtype=np.int64)\n",
    "            assert(x >= 2)\n",
    "\n",
    "            # Compute input and output shapes based on lowest-res feature map\n",
    "            output_shape = x\n",
    "            input_shape = x\n",
    "\n",
    "            # Extra conv\n",
    "            input_shape = input_shape + self.filter_size - 1\n",
    "\n",
    "            # Go from centre feature map through up- and downsampling blocks\n",
    "            for i in range(self.num_layers):\n",
    "                output_shape = 2*output_shape - 1 #Upsampling\n",
    "                output_shape = output_shape - self.merge_filter_size + 1 # Conv\n",
    "\n",
    "                input_shape = 2*input_shape - 1 # Decimation\n",
    "                if i < self.num_layers - 1:\n",
    "                    input_shape = input_shape + self.filter_size - 1 # Conv\n",
    "                else:\n",
    "                    input_shape = input_shape + self.input_filter_size - 1\n",
    "\n",
    "            # Output filters\n",
    "            output_shape = output_shape - self.output_filter_size + 1\n",
    "\n",
    "            input_shape = np.concatenate([[shape[0]], [input_shape], [self.num_channels]])\n",
    "            output_shape = np.concatenate([[shape[0]], [output_shape], [self.num_channels]])\n",
    "\n",
    "            return input_shape, output_shape\n",
    "        else:\n",
    "            return [shape[0], shape[1], self.num_channels], [shape[0], shape[1], self.num_channels]\n",
    "\n",
    "    def get_output(self, input, training, return_spectrogram=False, reuse=True):\n",
    "        '''\n",
    "        Creates symbolic computation graph of the U-Net for a given input batch\n",
    "        :param input: Input batch of mixtures, 3D tensor [batch_size, num_samples, num_channels]\n",
    "        :param reuse: Whether to create new parameter variables or reuse existing ones\n",
    "        :return: U-Net output: List of source estimates. Each item is a 3D tensor [batch_size, num_out_samples, num_channels]\n",
    "        '''\n",
    "        enc_outputs = list()\n",
    "        current_layer = input\n",
    "\n",
    "        # Down-convolution: Repeat strided conv\n",
    "        for i in range(self.num_layers):\n",
    "            current_layer = tf.keras.layers.Conv1D(self.num_initial_filters + (self.num_initial_filters * i), self.filter_size, strides=1, activation=LeakyReLU, padding=self.padding)(current_layer) # out = in - filter + 1\n",
    "            enc_outputs.append(current_layer)\n",
    "            current_layer = current_layer[:,::2,:] # Decimate by factor of 2 # out = (in-1)/2 + 1\n",
    "\n",
    "        current_layer = tf.keras.layers.Conv1D(self.num_initial_filters + (self.num_initial_filters * self.num_layers),self.filter_size,activation=LeakyReLU,padding=self.padding)(current_layer) # One more conv here since we need to compute features after last decimation\n",
    "\n",
    "        # Feature map here shall be X along one dimension\n",
    "\n",
    "        # Upconvolution\n",
    "        for i in range(self.num_layers):\n",
    "            #UPSAMPLING\n",
    "            current_layer = tf.expand_dims(current_layer, axis=1)\n",
    "            if self.upsampling == 'learned':\n",
    "                # Learned interpolation between two neighbouring time positions by using a convolution filter of width 2, and inserting the responses in the middle of the two respective inputs\n",
    "                current_layer = tf_baseline.learned_interpolation_layer(current_layer, self.padding, i)\n",
    "            else:\n",
    "                if self.context:\n",
    "                    current_layer = tf.image.resize_bilinear(current_layer, [1, current_layer.get_shape().as_list()[2] * 2 - 1], align_corners=True)\n",
    "                else:\n",
    "                    current_layer = tf.image.resize_bilinear(current_layer, [1, current_layer.get_shape().as_list()[2]*2]) # out = in + in - 1\n",
    "            current_layer = tf.squeeze(current_layer, axis=1)\n",
    "            # UPSAMPLING FINISHED\n",
    "\n",
    "            assert(enc_outputs[-i-1].get_shape().as_list()[1] == current_layer.get_shape().as_list()[1] or self.context) #No cropping should be necessary unless we are using context\n",
    "            current_layer = tf_baseline.crop_and_concat(enc_outputs[-i-1], current_layer, match_feature_dim=False)\n",
    "            current_layer = tf.layers.conv1d(current_layer, self.num_initial_filters + (self.num_initial_filters * (self.num_layers - i - 1)), self.merge_filter_size,\n",
    "                                             activation=LeakyReLU,\n",
    "                                             padding=self.padding)  # out = in - filter + 1\n",
    "\n",
    "        current_layer = tf_baseline.crop_and_concat(input, current_layer, match_feature_dim=False)\n",
    "\n",
    "        # Output layer\n",
    "        # Determine output activation function\n",
    "        if self.output_activation == \"tanh\":\n",
    "            out_activation = tf.tanh\n",
    "        elif self.output_activation == \"linear\":\n",
    "            out_activation = lambda x: tf_baseline.AudioClip(x, training)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.output_type == \"direct\":\n",
    "            return tf_baseline.independent_outputs(current_layer, self.source_names, self.num_channels, self.output_filter_size, self.padding, out_activation)\n",
    "        elif self.output_type == \"difference\":\n",
    "            cropped_input = tf_baseline.crop(input,current_layer.get_shape().as_list(), match_feature_dim=False)\n",
    "            return tf_baseline.difference_output(cropped_input, current_layer, self.source_names, self.num_channels, self.output_filter_size, self.padding, out_activation, training)\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator_class = UnetAudioSeparator(args)\n",
    "separator_func = separator_class.get_output\n",
    "separator_func(tf_tensor, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lg/85bmd5sn1pl9tcjvk2s7vtl00000gn/T/ipykernel_3218/3399627081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseparator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lg/85bmd5sn1pl9tcjvk2s7vtl00000gn/T/ipykernel_3218/2364315006.py\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(self, input, training, return_spectrogram, reuse)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Down-convolution: Repeat strided conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mcurrent_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_initial_filters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_initial_filters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# out = in - filter + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0menc_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcurrent_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Decimate by factor of 2 # out = (in-1)/2 + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'layers'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "pml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
